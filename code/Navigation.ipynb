{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Navigation\n",
    "\n",
    "---\n",
    "\n",
    "Train an single DRL Unity ML-agent to navigate the [Banana Collector](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#banana-collector) environment.\n",
    "\n",
    "---\n",
    "\n",
    "You can download the environment matching your operation system from one of the following links:\n",
    "* Linux: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Linux.zip)\n",
    "* Mac OSX: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana.app.zip)\n",
    "* Windows (32-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Windows_x86.zip)\n",
    "* Windows (64-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Windows_x86_64.zip)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 1. Start the Environment\n",
    "\n",
    "If necessary install following packages: **matplotlib**, **numpy**, **torch**, **unityagents**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If needed uncoment the following cell and install **matplotlib**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "# !{sys.executable} -m pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If needed uncoment the following cell and install **numpy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !{sys.executable} -m pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If needed uncoment the following cell and install **torch**. We will build the deep Q-Networks using torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If needed uncoment the following cell and install **unityagents**. This package is needed to run the downloaded Unity Environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install unityagents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  \n",
    "\n",
    "\n",
    "\n",
    "**_Before running the code cell below_**, change the `bananaApp` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bananaApp = '/path/to/banana environment/...'\n",
    "\n",
    "env = UnityEnvironment(file_name=bananaApp,worker_id=1,no_graphics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Banana Collector](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#banana-collector) is in general a multi agent environment. Here we train a single agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default environment brain\n",
    "envBrainName = env.brain_names[0]\n",
    "envBrain = env.brains[envBrainName]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Examine the State and Action Spaces\n",
    "\n",
    "Before starting with the training let's examine the environment.\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  \n",
    "\n",
    "At each time step, the agnet has four **actions** at its disposal:\n",
    "    - walk forward \n",
    "    - walk backward\n",
    "    - turn left\n",
    "    - turn right\n",
    "\n",
    "**The state space** has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  \n",
    "\n",
    "**The rewards** are \n",
    "\n",
    "    `+1` for collecting a yellow banana\n",
    "    `-1` for collecting a blue banana. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[envBrainName]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:\\t', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = envBrain.vector_action_space_size\n",
    "print('\\nNumber of actions:\\t', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "state_size = len(state)\n",
    "print('\\nStates have length:\\t', state_size)\n",
    "print('\\nStates look like:\\n\\n', state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train your agent\n",
    "\n",
    "First of all we need to import some dependencies and in particular the DrlAgent and DrlBrain classes. The later controlls the whole learning process parameters as well as the structure of the underlying Deep Q-Network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from agent import DrlAgent, DrlBrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define the training function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAgent(nrEpisondes=1100, maxT=10000, epsStart=1, epsEnd=0.01, epsDecay=0.999):\n",
    "    \"\"\"\n",
    "    Run training. \n",
    "    Args:\n",
    "        nrEpisondes: \n",
    "            Number of episodes the training should last\n",
    "        maxT:\n",
    "            Max munber of steps during a single episode\n",
    "        epsStart:\n",
    "            Value of epsilon at the beginning of trainig.\n",
    "        epsEnd:\n",
    "            Value of epsilon at the end of trainig. (min value of epsilon)\n",
    "        epsDecay:\n",
    "            Decay of epsilon after each episode\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Training started:\\n\")\n",
    "    \n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = epsStart/epsDecay            # initialize epsilon\n",
    "    eps0 = epsStart\n",
    "    \n",
    "    maxScore = 0\n",
    "    \n",
    "    for i_episode in range(1, nrEpisondes+1):\n",
    "        \n",
    "        env_info = env.reset(train_mode=True)[envBrainName]\n",
    "        state = env_info.vector_observations[0]   # get the initial state\n",
    "        \n",
    "        eps = max(epsEnd, epsDecay*eps) # decrease epsilon\n",
    "        \n",
    "        if i_episode % 100 == 0:\n",
    "            eps0 = eps\n",
    "            print(\"\\n\")\n",
    "        \n",
    "        score = 0              \n",
    "        for t in range(maxT):\n",
    "            action = agent.act(state, eps)\n",
    "                \n",
    "            env_info = env.step(action)[envBrainName]        # send the action to the environment\n",
    "            next_state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "                \n",
    "            \n",
    "            agent.experience(state, action, reward, next_state, done)\n",
    "                           \n",
    "                \n",
    "            state = next_state\n",
    "            score += reward\n",
    "            \n",
    "            if done:\n",
    "                break \n",
    "            \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        print('\\rEpisode {}\\tAverage Score: {:3.2f}\\tLast Score: {:3.2f}\\teps: {:1.4f} -> {:1.4f} '\\\n",
    "              .format(i_episode, np.mean(scores_window),score,eps0,eps), end=\"\")   \n",
    "        \n",
    "        if score>13 and score>maxScore:\n",
    "            agent.saveTrainingStatus('checkpoint.pth')\n",
    "            maxScore = score\n",
    "            \n",
    "    return scores\n",
    "\n",
    "\n",
    "def plotScores(scores,meanOver = 100):\n",
    "    \"\"\"\n",
    "    Plot the scores\n",
    "    \"\"\"\n",
    "    yLimMin = -5\n",
    "    scores = np.array(scores)\n",
    "    \n",
    "    runMean = np.convolve(scores, np.ones((meanOver,))/meanOver,mode='valid')[1:]\n",
    "\n",
    "    mean13 = np.argwhere(runMean>13) + meanOver\n",
    "    score13 = np.argwhere(scores>13)\n",
    "    \n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.plot(np.arange(len(scores)), scores)\n",
    "    plt.plot(np.arange(meanOver,len(scores)), runMean)\n",
    "    \n",
    "    plt.plot([0, len(scores)],[13,13],'r')\n",
    "    plt.plot([mean13[0],mean13[0]],[-5,13],'r')\n",
    "    plt.text(mean13[0]-100,yLimMin+1,str(mean13[0]),color = 'r')\n",
    "    \n",
    "    #plt.scatter(score13,scores[score13],color='r')\n",
    "    \n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.xlim([0,len(scores)])\n",
    "    plt.ylim([yLimMin,27])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the training for different agent configurations:\n",
    "\n",
    "To approximate the state action value function we will use a deep Q-Network with `[64,32,16]` hidden layers. \n",
    "\n",
    "The (local) Q-network will be updated every `dqnUpdatePace = 5` steps using a batch gradient descent with learning rate `learningRate = 1e-4` and batch size `batchSize = 64`. \n",
    "\n",
    "The target network will be _softly_ updated with a parameter _Tau_ equal to `targetDqnUpdatePace = 5e-4`: \n",
    "\n",
    "    TARGET <--- (1-Tau) * TARGET + Tau * LOCAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agentBrain = DrlBrain(stateSize=state_size, actionSize=action_size, \n",
    "             hiddenLayers=[64,32,16],\n",
    "             gamma = 0.99,\n",
    "             learningRate = 1e-4,     \n",
    "             dropProb = .0,\n",
    "             dqnUpdatePace = 5, \n",
    "             targetDqnUpdatePace = 5e-4,\n",
    "             bufferSize = int(1e6),\n",
    "             batchSize = 64,\n",
    "             batchEpochs = 1,\n",
    "             seed = 0)\n",
    " \n",
    "agent = DrlAgent(agentBrain)\n",
    "\n",
    "scores = trainAgent(epsStart=1, epsEnd=0.01, epsDecay=0.97)\n",
    "plotScores(scores)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "   Instead of a _soft_ update, let us consider a _hard_ update of the target network after `targetDqnUpdatePace = 299` steps which corresponds to the length of a single episode. \n",
    "   \n",
    "   I.e. the target network is not updated smoothly after each update of the local network, but is completely overwritten every `299` steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agentBrain = DrlBrain(stateSize=state_size, actionSize=action_size, \n",
    "             hiddenLayers=[64,32,16],\n",
    "             gamma = 0.99,\n",
    "             learningRate = 1e-4,     \n",
    "             dropProb = .0,\n",
    "             dqnUpdatePace = 5, \n",
    "             targetDqnUpdatePace = 299,  \n",
    "             bufferSize = int(1e6),\n",
    "             batchSize = 64,\n",
    "             batchEpochs = 1, \n",
    "             seed = 0)\n",
    " \n",
    "agent = DrlAgent(agentBrain)\n",
    "\n",
    "scores = trainAgent(epsStart=1, epsEnd=0.01, epsDecay=0.97)\n",
    "plotScores(scores)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now the local network was updated every `dqnUpdatePace = 5` steps using a batch gradient descent (batch size of `batchSize = 64`) with a single _gradient step_ (single epoch). \n",
    "\n",
    "Let us now use a batch gradient descent of the same batch size but instead of making a single _gradient step_ each time the network is updated we will make `batchEpochs = 5` steps. I.e. we will run each time the gradient descent for five epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agentBrain = DrlBrain(stateSize=state_size, actionSize=action_size, \n",
    "             hiddenLayers=[64,32,16],\n",
    "             gamma = 0.99,\n",
    "             learningRate = 1e-4,     \n",
    "             dropProb = .0,\n",
    "             dqnUpdatePace = 5, \n",
    "             targetDqnUpdatePace = 299, \n",
    "             bufferSize = int(1e6),\n",
    "             batchSize = 64,\n",
    "             batchEpochs = 5, \n",
    "             seed = 0)\n",
    " \n",
    "agent = DrlAgent(agentBrain)\n",
    "\n",
    "scores = trainAgent(epsStart=1, epsEnd=0.01, epsDecay=0.97)\n",
    "plotScores(scores)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deal with potential overfitting (enhanced by the higher number of epochs) we can introduce drop-out regularization with  a drop-out probability of `dropProb = 0.1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agentBrain = DrlBrain(stateSize=state_size, actionSize=action_size, \n",
    "             hiddenLayers=[64,32,16],\n",
    "             gamma = 0.99,\n",
    "             learningRate = 1e-4,     \n",
    "             dropProb = 0.1, \n",
    "             dqnUpdatePace = 5, \n",
    "             targetDqnUpdatePace = 299, \n",
    "             bufferSize = int(1e6),\n",
    "             batchSize = 64,\n",
    "             batchEpochs = 5, \n",
    "             seed = 0)\n",
    " \n",
    "agent = DrlAgent(agentBrain)\n",
    "\n",
    "scores = trainAgent(epsStart=1, epsEnd=0.01, epsDecay=0.97)\n",
    "plotScores(scores)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
